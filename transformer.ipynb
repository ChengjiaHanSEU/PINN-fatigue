{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0925897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import time\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity\n",
    "import time\n",
    "from skimage import filters, img_as_ubyte,morphology,measure\n",
    "from scipy.ndimage import label\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd24d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "################数据导入###################\n",
    "degree5_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-200-5.csv\").iloc[:,1:]\n",
    "degree5_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-300-5.csv\").iloc[:,1:]\n",
    "degree5_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-400-5.csv\").iloc[:,1:]\n",
    "degree15_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-200-5.csv\").iloc[:,1:]\n",
    "degree15_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-300-5.csv\").iloc[:,1:]\n",
    "degree15_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-400-5.csv\").iloc[:,1:]\n",
    "degree25_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-200-5.csv\").iloc[:,1:]\n",
    "degree25_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-300-5.csv\").iloc[:,1:]\n",
    "degree25_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-400-5.csv\").iloc[:,1:] ###测试\n",
    "\n",
    "step_max = int(np.array([degree5_200_5.max(),degree5_300_5.max(),degree5_400_5.max(),\n",
    "            degree15_200_5.max(),degree15_300_5.max(),degree15_400_5.max(),\n",
    "            degree25_200_5.max(),degree25_300_5.max(),degree25_400_5.max(),]).max())+500\n",
    "####建立训练集\n",
    "###lenght=15, 预测1个,随即时间间隔+物理约束\n",
    "####5,15,25三类，200，300，400三类\n",
    "X_data = [] \n",
    "X_data_increase = []\n",
    "X_data_factor = [] \n",
    "Y_data = []\n",
    "\n",
    "########5度###############\n",
    "for i in range (len(degree5_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_200_5.iloc[i:i+15,1])/step_max,np.array(degree5_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[1,0,0]])\n",
    "    X_data_increase.append(degree5_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_300_5.iloc[i:i+15,1])/step_max,np.array(degree5_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,1,0]])\n",
    "    X_data_increase.append(degree5_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_400_5.iloc[i:i+15,1])/step_max,np.array(degree5_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,0,1]])\n",
    "    X_data_increase.append(degree5_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_400_5.iloc[i+15,0])\n",
    "\n",
    "########15度###############\n",
    "for i in range (len(degree15_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_200_5.iloc[i:i+15,1])/step_max,np.array(degree15_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[1,0,0]])\n",
    "    X_data_increase.append(degree15_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_300_5.iloc[i:i+15,1])/step_max,np.array(degree15_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,1,0]])\n",
    "    X_data_increase.append(degree15_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_400_5.iloc[i:i+15,1])/step_max,np.array(degree15_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,0,1]])\n",
    "    X_data_increase.append(degree15_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_400_5.iloc[i+15,0])\n",
    "\n",
    "########25度###############\n",
    "for i in range (len(degree25_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_200_5.iloc[i:i+15,1])/step_max,np.array(degree25_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[1,0,0]])\n",
    "    X_data_increase.append(degree25_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree25_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_300_5.iloc[i:i+15,1])/step_max,np.array(degree25_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[0,1,0]])\n",
    "    X_data_increase.append(degree25_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_300_5.iloc[i+15,0])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "X_data_increase = np.array(X_data_increase)\n",
    "Y_data = np.array(Y_data)\n",
    "X_data_factor = np.array(X_data_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 模型########\n",
    "def scaled_dot_product_attention(Q, K, V,\n",
    "                                 dropout_rate=0.1,\n",
    "                                 training=True,\n",
    "                                 scope=\"scaled_dot_product_attention\"):\n",
    "    '''\n",
    "    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n",
    "    K: Packed keys. 3d tensor. [N, T_k, d_k].\n",
    "    V: Packed values. 3d tensor. [N, T_k, d_v].\n",
    "    key_masks: A 2d tensor with shape of [N, key_seqlen]\n",
    "    causality: If True, applies masking for future blinding\n",
    "    dropout_rate: A floating point number of [0, 1].\n",
    "    training: boolean for controlling droput\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "    '''\n",
    "    d_k = Q.get_shape().as_list()[-1]\n",
    "\n",
    "    # dot product\n",
    "    outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n",
    "\n",
    "    # scale\n",
    "    outputs /= d_k ** 0.5\n",
    "\n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(outputs)\n",
    "\n",
    "    # dropout\n",
    "    outputs = tf.keras.layers.Dropout(dropout_rate)(outputs)\n",
    "\n",
    "    # weighted sum (context vectors)\n",
    "    outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "def multihead_attention(queries, keys, values,\n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0.1,\n",
    "                        training=True,\n",
    "                        scope=\"multihead_attention\"):\n",
    "    '''\n",
    "    queries: A 3d tensor with shape of [N, T_q, d_model].\n",
    "    keys: A 3d tensor with shape of [N, T_k, d_model].\n",
    "    values: A 3d tensor with shape of [N, T_k, d_model].\n",
    "    key_masks: A 2d tensor with shape of [N, key_seqlen]\n",
    "    num_heads: An int. Number of heads.\n",
    "    dropout_rate: A floating point number.\n",
    "    training: Boolean. Controller of mechanism for dropout.\n",
    "    causality: Boolean. If true, units that reference the future are masked.\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    d_model = queries.get_shape().as_list()[-1]\n",
    "    # Linear projections\n",
    "    Q = tf.keras.layers.Dense(d_model)(queries) # (N, T_q, d_model) \n",
    "    K = tf.keras.layers.Dense(d_model)(keys) # (N, T_k, d_model)\n",
    "    V = tf.keras.layers.Dense(d_model)(values)# (N, T_v, d_model)\n",
    "    \n",
    "    # Split and concat\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_v, d_model/h)\n",
    "\n",
    "    # Attention\n",
    "    outputs =  scaled_dot_product_attention(Q_, K_, V_, dropout_rate, training)\n",
    "\n",
    "    # Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "\n",
    "    # Residual connection\n",
    "    outputs = tf.keras.layers.add([queries,outputs])\n",
    "\n",
    "    # Normalize\n",
    "    outputs = tf.keras.layers.LayerNormalization()(outputs)\n",
    " \n",
    "    return outputs\n",
    "\n",
    "def ff(inputs, num_units, scope=\"positionwise_feedforward\"):\n",
    "    '''\n",
    "    inputs: A 3d tensor with shape of [N, T, C].\n",
    "    num_units: A list of two integers.\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    # Inner layer\n",
    "    outputs = tf.keras.layers.Dense(num_units[0], activation=tf.nn.relu)(inputs) \n",
    "\n",
    "    # Outer layer\n",
    "    outputs = tf.keras.layers.Dense(num_units[1])(outputs)\n",
    "\n",
    "    # Residual connection\n",
    "    outputs = tf.keras.layers.concatenate([outputs, inputs],axis=-1)\n",
    "\n",
    "    # Normalize\n",
    "    outputs = tf.keras.layers.LayerNormalization()(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'dtype': self.dtype,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def Fatigue_transformer():\n",
    "    History_input = tf.keras.layers.Input(shape=(15,2)) #历史数据，15个点\n",
    "    Predicted_input_x = tf.keras.layers.Input(shape=(1)) #预测点的x坐标\n",
    "    Class_curve = tf.keras.layers.Input(shape=(2,3)) #判定曲线类型\n",
    "    \n",
    "    encoder_in = tf.keras.layers.Dense(64)(History_input) #(None,15,64)\n",
    "    layer1 = multihead_attention(encoder_in,encoder_in,encoder_in)\n",
    "    layer2 = multihead_attention(layer1, layer1, layer1)\n",
    "    layer3 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu)(layer2)\n",
    "    layer4 = tf.keras.layers.Flatten()(layer3)\n",
    "    Layer5 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(layer4)\n",
    "    ####增量预测\n",
    "    Increase1 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu)(Predicted_input_x)\n",
    "    ####类别\n",
    "    Onehot1 = tf.keras.layers.Dense(64)(Class_curve)\n",
    "    Onehot2 = tf.keras.layers.LSTM(64,activation='tanh',return_sequences=False)(Onehot1)\n",
    "    ####拼接\n",
    "    Dense1 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(tf.concat([Layer5,Increase1,Onehot2],-1))\n",
    "    Dense2 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense1)\n",
    "    LayNorm = tf.keras.layers.LayerNormalization()(Dense2)\n",
    "    Dense3 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(LayNorm)\n",
    "    Output = tf.keras.layers.Dense(1,activation=\"relu\")(Dense3) ###输出预测的百分比\n",
    " \n",
    "    #固化模型 \n",
    "    model=tf.keras.models.Model(inputs=[History_input,Predicted_input_x,Class_curve],outputs=Output)   \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a499a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Fatigue_transformer()\n",
    "model.compile( optimizer='adam',loss=tf.keras.losses.MeanAbsoluteError())\n",
    "History = model.fit(x=[X_data,X_data_increase,X_data_factor],y=Y_data,batch_size=4,epochs=100)\n",
    "model.save_weights(\"./Fatigue_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6bec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####训练########\n",
    "model = Fatigue_transformer()\n",
    "model.load_weights(\"./Fatigue_transformer\")\n",
    "History=[]\n",
    "Batch_size=4\n",
    "optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "for epoch in range(0,50,1):\n",
    "    count=0\n",
    "    Average_loss=0\n",
    "    MinEpochs = int(len(X_data)/Batch_size)\n",
    "    new_list = np.array([i for i in range (len(X_data))])\n",
    "    np.random.shuffle(new_list)\n",
    "    for j in range(MinEpochs):\n",
    "        count+=1\n",
    "        tem_list = new_list[j*Batch_size:(j+1)*Batch_size]\n",
    "        train_X_data = X_data[tem_list]\n",
    "        train_X_data_increase = X_data_increase[tem_list]\n",
    "        train_X_data_factor = X_data_factor[tem_list]\n",
    "        train_Y_data = Y_data[tem_list]\n",
    "        with tf.GradientTape() as tape:\n",
    "            Predicted_Y = model([train_X_data,train_X_data_increase, train_X_data_factor]) \n",
    "            Loss = tf.keras.losses.MAE(train_Y_data,Predicted_Y)\n",
    "            \n",
    "            gradients = tape.gradient(Loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  \n",
    "            Average_loss=Average_loss + tf.reduce_mean(Loss)\n",
    "    Average_loss = Average_loss/count\n",
    "    History.append([epoch, Average_loss])\n",
    "    tf.print(\"=>Epoch%4d  loss:%4.6f\" %(epoch, Average_loss))   \n",
    "\n",
    "    model.save_weights(\"./Fatigue_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "model = Fatigue_transformer()\n",
    "model.load_weights(\"./Fatigue_transformer\")\n",
    "X_data_test = [] \n",
    "X_data_increase_test = []\n",
    "X_data_factor_test = [] \n",
    "Y_data_test = []\n",
    "for i in range (len(degree25_400_5)-15):\n",
    "    X_data_test.append(np.transpose([np.array(degree25_400_5.iloc[i:i+15,1])/step_max,np.array(degree25_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor_test.append([[0,0,1],[0,0,1]])\n",
    "    X_data_increase_test.append(degree25_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data_test.append(degree25_400_5.iloc[i+15,0])\n",
    "\n",
    "X_data_test = np.array(X_data_test) \n",
    "X_data_increase_test = np.array(X_data_increase_test)\n",
    "X_data_factor_test = np.array(X_data_factor_test)\n",
    "Y_data_test = np.array(Y_data_test)\n",
    "    \n",
    "predicted_results=[]\n",
    "for i in range(len(X_data_test)):\n",
    "    predicted_results.append(model([X_data_test[i:i+1],X_data_increase_test[i:i+1],X_data_factor_test[i:i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.squeeze(Y_data_test),np.squeeze(predicted_results))\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####绘制曲线######\n",
    "model = Fatigue_transformer()\n",
    "model.load_weights(\"./Fatigue_transformer\")\n",
    "initial_data_x = np.zeros(len(degree25_400_5))\n",
    "initial_data_x[0:20] = degree25_400_5.iloc[0:20,0]\n",
    "initial_data_inc = degree25_400_5.iloc[:,1]/step_max\n",
    "for i in range(20,len(initial_data_inc)):\n",
    "    X_data_test = np.transpose([np.array(degree25_400_5.iloc[i-15:i,1])/step_max,np.array(degree25_400_5.iloc[i-15:i,0])])[np.newaxis,...]\n",
    "    X_data_increase_test = np.array(degree25_400_5.iloc[i,1]/step_max)[np.newaxis,...]\n",
    "    X_data_factor_test_test = np.array([[0,0,1],[0,0,1]])[np.newaxis,...]\n",
    "    initial_data_x[i]= model([X_data_test,X_data_increase_test,X_data_factor_test_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(initial_data_inc*step_max,initial_data_x)\n",
    "plt.plot(initial_data_inc*step_max,degree25_400_5.iloc[:,0])\n",
    "plt.show()\n",
    "Ontest_set = np.concatenate((np.array(initial_data_inc*step_max)[...,np.newaxis],\n",
    "                             np.array(initial_data_x)[...,np.newaxis],\n",
    "                             np.array(degree25_400_5.iloc[:,0])[...,np.newaxis]),-1)\n",
    "Ontest_set = pd.DataFrame(Ontest_set)\n",
    "Ontest_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/transformer_testset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "model = Fatigue_transformer()\n",
    "model.load_weights(\"./Fatigue_transformer\")\n",
    "    \n",
    "predicted_results=[]\n",
    "for i in range(len(X_data)):\n",
    "    predicted_results.append(model([X_data[i:i+1],X_data_increase[i:i+1],X_data_factor[i:i+1]]))\n",
    "    \n",
    "ON_train_set = np.concatenate((np.squeeze(Y_data)[...,np.newaxis],np.squeeze(predicted_results)[...,np.newaxis]),-1)\n",
    "ON_train_set = pd.DataFrame(ON_train_set)\n",
    "ON_train_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/transformer_trainset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32effa01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
