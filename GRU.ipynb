{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb94b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import time\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity\n",
    "import time\n",
    "from skimage import filters, img_as_ubyte,morphology,measure\n",
    "from scipy.ndimage import label\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################数据导入###################\n",
    "degree5_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-200-5.csv\").iloc[1:,1:]\n",
    "degree5_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-300-5.csv\").iloc[1:,1:]\n",
    "degree5_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-400-5.csv\").iloc[1:,1:]\n",
    "degree15_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-200-5.csv\").iloc[1:,1:]\n",
    "degree15_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-300-5.csv\").iloc[1:,1:]\n",
    "degree15_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-400-5.csv\").iloc[1:,1:]\n",
    "degree25_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-200-5.csv\").iloc[1:,1:]\n",
    "degree25_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-300-5.csv\").iloc[1:,1:]\n",
    "degree25_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-400-5.csv\").iloc[1:,1:] ###测试\n",
    "\n",
    "step_max = int(np.array([degree5_200_5.max(),degree5_300_5.max(),degree5_400_5.max(),\n",
    "            degree15_200_5.max(),degree15_300_5.max(),degree15_400_5.max(),\n",
    "            degree25_200_5.max(),degree25_300_5.max(),degree25_400_5.max(),]).max())+500\n",
    "####建立训练集\n",
    "###lenght=15, 预测1个,随即时间间隔+物理约束\n",
    "####5,15,25三类，200，300，400三类\n",
    "X_data = [] \n",
    "X_data_increase = []\n",
    "X_data_factor = [] \n",
    "Y_data = []\n",
    "\n",
    "########5度###############\n",
    "for i in range (len(degree5_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_200_5.iloc[i:i+15,1])/step_max,np.array(degree5_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[1,0,0]])\n",
    "    X_data_increase.append(degree5_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_300_5.iloc[i:i+15,1])/step_max,np.array(degree5_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,1,0]])\n",
    "    X_data_increase.append(degree5_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_400_5.iloc[i:i+15,1])/step_max,np.array(degree5_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,0,1]])\n",
    "    X_data_increase.append(degree5_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_400_5.iloc[i+15,0])\n",
    "\n",
    "########15度###############\n",
    "for i in range (len(degree15_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_200_5.iloc[i:i+15,1])/step_max,np.array(degree15_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[1,0,0]])\n",
    "    X_data_increase.append(degree15_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_300_5.iloc[i:i+15,1])/step_max,np.array(degree15_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,1,0]])\n",
    "    X_data_increase.append(degree15_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_400_5.iloc[i:i+15,1])/step_max,np.array(degree15_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,0,1]])\n",
    "    X_data_increase.append(degree15_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_400_5.iloc[i+15,0])\n",
    "\n",
    "########25度###############\n",
    "for i in range (len(degree25_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_200_5.iloc[i:i+15,1])/step_max,np.array(degree25_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[1,0,0]])\n",
    "    X_data_increase.append(degree25_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree25_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_300_5.iloc[i:i+15,1])/step_max,np.array(degree25_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[0,1,0]])\n",
    "    X_data_increase.append(degree25_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_300_5.iloc[i+15,0])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "X_data_increase = np.array(X_data_increase)\n",
    "Y_data = np.array(Y_data)\n",
    "X_data_factor = np.array(X_data_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "######GRU############\n",
    "def Fatigue_GRU():\n",
    "    History_input = tf.keras.layers.Input(shape=(15,2)) #历史数据，15个点\n",
    "    Predicted_input_x = tf.keras.layers.Input(shape=(1)) #预测点的x坐标\n",
    "    Class_curve = tf.keras.layers.Input(shape=(2,3)) #判定曲线类型\n",
    "    encoder_in = tf.keras.layers.Dense(128)(History_input)\n",
    "    GRU1 = tf.keras.layers.GRU(128,activation='tanh',return_sequences=True)(encoder_in)\n",
    "    GRU2 = tf.keras.layers.GRU(128,activation='tanh',return_sequences=False)(GRU1)\n",
    "    LayerNorm = tf.keras.layers.LayerNormalization()(GRU2)\n",
    "    Dense1 = tf.keras.layers.Dense(128,activation=tf.nn.leaky_relu)(LayerNorm)\n",
    "    Densex1 = tf.keras.layers.Dense(64)(Predicted_input_x)\n",
    "    Onehot1 = tf.keras.layers.Dense(64)(Class_curve)\n",
    "    Onehot2 = tf.keras.layers.LSTM(64,activation='tanh',return_sequences=False)(Onehot1)\n",
    "    \n",
    "    Dense_s1 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(tf.concat([Dense1,Densex1,Onehot2],-1))\n",
    "    Dense_s2 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense_s1)\n",
    "    Output = tf.keras.layers.Dense(1,activation='relu')(Dense_s2) ###输出预测\n",
    "    \n",
    "    #固化模型 \n",
    "    model=tf.keras.models.Model(inputs=[History_input,Predicted_input_x,Class_curve],outputs=Output)   \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4832ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Fatigue_GRU()\n",
    "model.compile( optimizer='adam',loss=tf.keras.losses.MeanAbsoluteError())\n",
    "History = model.fit(x=[X_data,X_data_increase,X_data_factor],y=Y_data,batch_size=4,epochs=100)\n",
    "model.save_weights(\"./Fatigue_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b24f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####训练########\n",
    "model = Fatigue_GRU()\n",
    "model.load_weights(\"./Fatigue_GRU\")\n",
    "History=[]\n",
    "Batch_size=4\n",
    "optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "for epoch in range(0,50,1):\n",
    "    count=0\n",
    "    Average_loss=0\n",
    "    MinEpochs = int(len(X_data)/Batch_size)\n",
    "    new_list = np.array([i for i in range (len(X_data))])\n",
    "    np.random.shuffle(new_list)\n",
    "    for j in range(MinEpochs):\n",
    "        count+=1\n",
    "        tem_list = new_list[j*Batch_size:(j+1)*Batch_size]\n",
    "        train_X_data = X_data[tem_list]\n",
    "        train_X_data_increase = X_data_increase[tem_list]\n",
    "        train_X_data_factor = X_data_factor[tem_list]\n",
    "        train_Y_data = Y_data[tem_list]\n",
    "        with tf.GradientTape() as tape:\n",
    "            Predicted_Y = model([train_X_data,train_X_data_increase,train_X_data_factor]) \n",
    "            Loss = tf.keras.losses.MAE(train_Y_data,Predicted_Y)\n",
    "            \n",
    "            gradients = tape.gradient(Loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  \n",
    "            Average_loss=Average_loss + tf.reduce_mean(Loss)\n",
    "    Average_loss = Average_loss/count\n",
    "    History.append([epoch, Average_loss])\n",
    "    tf.print(\"=>Epoch%4d  loss:%4.6f\" %(epoch, Average_loss))   \n",
    "\n",
    "    model.save_weights(\"./Fatigue_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1931698",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "model  = Fatigue_GRU()\n",
    "model.load_weights(\"./Fatigue_GRU\")\n",
    "X_data_test = [] \n",
    "X_data_increase_test = []\n",
    "X_data_factor_test = [] \n",
    "Y_data_test = []\n",
    "for i in range (len(degree25_400_5)-15):\n",
    "    X_data_test.append(np.transpose([np.array(degree25_400_5.iloc[i:i+15,1])/step_max,np.array(degree25_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor_test.append([[0,0,1],[0,0,1]])\n",
    "    X_data_increase_test.append(degree25_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data_test.append(degree25_400_5.iloc[i+15,0])\n",
    "\n",
    "X_data_test = np.array(X_data_test) \n",
    "X_data_increase_test = np.array(X_data_increase_test)\n",
    "X_data_factor_test = np.array(X_data_factor_test)\n",
    "Y_data_test = np.array(Y_data_test)\n",
    "    \n",
    "predicted_results=[]\n",
    "for i in range(len(X_data_test)):\n",
    "    predicted_results.append(model([X_data_test[i:i+1],X_data_increase_test[i:i+1],X_data_factor_test[i:i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27961173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.squeeze(Y_data_test),np.squeeze(predicted_results))\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####绘制曲线######\n",
    "model  = Fatigue_GRU()\n",
    "model.load_weights(\"./Fatigue_GRU\")\n",
    "initial_data_x = np.zeros(len(degree25_400_5))\n",
    "initial_data_x[0:20] = degree25_400_5.iloc[0:20,0]\n",
    "initial_data_inc = degree25_400_5.iloc[:,1]/step_max\n",
    "for i in range(20,len(initial_data_inc)):\n",
    "    X_data_test = np.transpose([np.array(degree25_400_5.iloc[i-15:i,1])/step_max,np.array(degree25_400_5.iloc[i-15:i,0])])[np.newaxis,...]\n",
    "    X_data_increase_test = np.array(degree25_400_5.iloc[i,1]/step_max)[np.newaxis,...]\n",
    "    X_data_factor_test_test = np.array([[0,0,1],[0,0,1]])[np.newaxis,...]\n",
    "    initial_data_x[i]= model([X_data_test,X_data_increase_test,X_data_factor_test_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(initial_data_inc*step_max,initial_data_x)\n",
    "plt.plot(initial_data_inc*step_max,degree25_400_5.iloc[:,0])\n",
    "plt.show()\n",
    "Ontest_set = np.concatenate((np.array(initial_data_inc*step_max)[...,np.newaxis],\n",
    "                             np.array(initial_data_x)[...,np.newaxis],\n",
    "                             np.array(degree25_400_5.iloc[:,0])[...,np.newaxis]),-1)\n",
    "Ontest_set = pd.DataFrame(Ontest_set)\n",
    "Ontest_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/GRU_testset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742daf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "model  = Fatigue_GRU()\n",
    "model.load_weights(\"./Fatigue_GRU\")\n",
    "    \n",
    "predicted_results=[]\n",
    "for i in range(len(X_data)):\n",
    "    predicted_results.append(model([X_data[i:i+1],X_data_increase[i:i+1],X_data_factor[i:i+1]]))\n",
    "    \n",
    "ON_train_set = np.concatenate((np.squeeze(Y_data)[...,np.newaxis],np.squeeze(predicted_results)[...,np.newaxis]),-1)\n",
    "ON_train_set = pd.DataFrame(ON_train_set)\n",
    "ON_train_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/GRU_trainset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae318f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
