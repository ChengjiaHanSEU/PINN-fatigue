{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59574b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import time\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity\n",
    "import time\n",
    "from skimage import filters, img_as_ubyte,morphology,measure\n",
    "from scipy.ndimage import label\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b914c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 模型########\n",
    "def scaled_dot_product_attention(Q, K, V,\n",
    "                                 dropout_rate=0.1,\n",
    "                                 training=True,\n",
    "                                 scope=\"scaled_dot_product_attention\"):\n",
    "    '''\n",
    "    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n",
    "    K: Packed keys. 3d tensor. [N, T_k, d_k].\n",
    "    V: Packed values. 3d tensor. [N, T_k, d_v].\n",
    "    key_masks: A 2d tensor with shape of [N, key_seqlen]\n",
    "    causality: If True, applies masking for future blinding\n",
    "    dropout_rate: A floating point number of [0, 1].\n",
    "    training: boolean for controlling droput\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "    '''\n",
    "    d_k = Q.get_shape().as_list()[-1]\n",
    "\n",
    "    # dot product\n",
    "    outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n",
    "\n",
    "    # scale\n",
    "    outputs /= d_k ** 0.5\n",
    "\n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(outputs)\n",
    "\n",
    "    # dropout\n",
    "    outputs = tf.keras.layers.Dropout(dropout_rate)(outputs)\n",
    "\n",
    "    # weighted sum (context vectors)\n",
    "    outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "def multihead_attention(queries, keys, values,\n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0.1,\n",
    "                        training=True,\n",
    "                        scope=\"multihead_attention\"):\n",
    "    '''\n",
    "    queries: A 3d tensor with shape of [N, T_q, d_model].\n",
    "    keys: A 3d tensor with shape of [N, T_k, d_model].\n",
    "    values: A 3d tensor with shape of [N, T_k, d_model].\n",
    "    key_masks: A 2d tensor with shape of [N, key_seqlen]\n",
    "    num_heads: An int. Number of heads.\n",
    "    dropout_rate: A floating point number.\n",
    "    training: Boolean. Controller of mechanism for dropout.\n",
    "    causality: Boolean. If true, units that reference the future are masked.\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    d_model = queries.get_shape().as_list()[-1]\n",
    "    # Linear projections\n",
    "    Q = tf.keras.layers.Dense(d_model)(queries) # (N, T_q, d_model) \n",
    "    K = tf.keras.layers.Dense(d_model)(keys) # (N, T_k, d_model)\n",
    "    V = tf.keras.layers.Dense(d_model)(values)# (N, T_v, d_model)\n",
    "    \n",
    "    # Split and concat\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_v, d_model/h)\n",
    "\n",
    "    # Attention\n",
    "    outputs =  scaled_dot_product_attention(Q_, K_, V_, dropout_rate, training)\n",
    "\n",
    "    # Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "\n",
    "    # Residual connection\n",
    "    outputs = tf.keras.layers.add([queries,outputs])\n",
    "\n",
    "    # Normalize\n",
    "    outputs = tf.keras.layers.LayerNormalization()(outputs)\n",
    " \n",
    "    return outputs\n",
    "\n",
    "def ff(inputs, num_units, scope=\"positionwise_feedforward\"):\n",
    "    '''\n",
    "    inputs: A 3d tensor with shape of [N, T, C].\n",
    "    num_units: A list of two integers.\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    # Inner layer\n",
    "    outputs = tf.keras.layers.Dense(num_units[0], activation=tf.nn.relu)(inputs) \n",
    "\n",
    "    # Outer layer\n",
    "    outputs = tf.keras.layers.Dense(num_units[1])(outputs)\n",
    "\n",
    "    # Residual connection\n",
    "    outputs = tf.keras.layers.concatenate([outputs, inputs],axis=-1)\n",
    "\n",
    "    # Normalize\n",
    "    outputs = tf.keras.layers.LayerNormalization()(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'dtype': self.dtype,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def Fatigue_PINN():\n",
    "    History_input = tf.keras.layers.Input(shape=(15,2)) #历史数据，15个点\n",
    "    Predicted_input_x = tf.keras.layers.Input(shape=(1)) #预测点的x坐标\n",
    "    Class_curve = tf.keras.layers.Input(shape=(2,3)) #判定曲线类型\n",
    "    #####Path1:deep predciton for value \n",
    "    ####Attenation部分\n",
    "    encoder_in = tf.keras.layers.Dense(64)(History_input) #(None,15,64)\n",
    "    decoder_in = tf.keras.layers.Dense(64)(History_input) #(None,15,64)\n",
    "    \"\"\"\n",
    "    positional_encoding = PositionalEncoding(max_steps=15, max_dims=64)\n",
    "    encoder_in = positional_encoding(encoder_in)\n",
    "    decoder_in = positional_encoding(decoder_in)\n",
    "    \"\"\"\n",
    "    layer1 = multihead_attention(encoder_in,encoder_in,decoder_in)\n",
    "    layer2 = ff(layer1, [128,128])\n",
    "    \n",
    "    layer3 = multihead_attention(encoder_in, decoder_in, decoder_in)\n",
    "    layer4 = multihead_attention(layer3, layer2, decoder_in)\n",
    "    \n",
    "    layer5 = ff(layer4, [128,128])\n",
    "    layer6 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu)(layer5)\n",
    "    layer6 = tf.keras.layers.Flatten()(layer6)\n",
    "    Layer7 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(layer6)\n",
    "    ####增量预测\n",
    "    Increase1 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu)(Predicted_input_x)\n",
    "    ####拼接\n",
    "    Dense1 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(tf.concat([Layer7,Increase1],-1))\n",
    "    Dense2 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense1)\n",
    "    LayNorm = tf.keras.layers.LayerNormalization()(Dense2)\n",
    "    Dense3 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(LayNorm)\n",
    "    Output1 = tf.keras.layers.Dense(1,activation=\"sigmoid\")(Dense3) ###输出预测的百分比\n",
    "    \n",
    "    #####Path2:shallow predciton for range \n",
    "    Lstm1 = tf.keras.layers.LSTM(64,activation='tanh',return_sequences=True)(encoder_in)\n",
    "    Lstm2 = tf.keras.layers.LSTM(64,activation='tanh',return_sequences=False)(Lstm1)\n",
    "    Onehot1 = tf.keras.layers.Dense(64)(Class_curve)\n",
    "    Onehot2 = tf.keras.layers.LSTM(64,activation='tanh',return_sequences=False)(Onehot1)\n",
    "    Dense_s1 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(tf.concat([Lstm2,Increase1,Onehot2],-1))\n",
    "    Dense_s2 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense_s1)\n",
    "    Output2 = tf.keras.layers.Dense(2,activation='relu')(Dense_s2) ###输出预测的范围上下限\n",
    "    \n",
    "    #固化模型 \n",
    "    model=tf.keras.models.Model(inputs=[History_input,Predicted_input_x,Class_curve],outputs=[Output1,Output2])   \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96917de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####训练########\n",
    "model = Fatigue_PINN()\n",
    "model.load_weights(\"./Fatigue_PINN\")\n",
    "History=[]\n",
    "Batch_size=4\n",
    "optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "for epoch in range(0,100,1):\n",
    "    count=0\n",
    "    Average_loss1=0\n",
    "    Average_loss2=0\n",
    "    MinEpochs = int(len(X_data)/Batch_size)\n",
    "    new_list = np.array([i for i in range (len(X_data))])\n",
    "    np.random.shuffle(new_list)\n",
    "    for j in range(MinEpochs):\n",
    "        count+=1\n",
    "        tem_list = new_list[j*Batch_size:(j+1)*Batch_size]\n",
    "        train_X_data = X_data[tem_list]\n",
    "        train_X_data_increase = X_data_increase[tem_list]\n",
    "        train_X_data_factor = X_data_factor[tem_list]\n",
    "        train_Y_data = Y_data[tem_list]\n",
    "        train_Y_data_con = Y_data_con[tem_list]\n",
    "        with tf.GradientTape() as tape:\n",
    "            Output1,Output2 = model([train_X_data,train_X_data_increase,train_X_data_factor]) \n",
    "            Predicted_Y = Output2[:,0]+(Output2[:,1]-Output2[:,0])*Output1 \n",
    "            Loss1 = tf.keras.losses.MAE(train_Y_data,Predicted_Y)\n",
    "            Loss2 = tf.keras.losses.MAE(train_Y_data_con,Output2)\n",
    "            Loss  = 10 * Loss1 + Loss2\n",
    "            \n",
    "            gradients = tape.gradient(Loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  \n",
    "            Average_loss1=Average_loss1 + tf.reduce_mean(Loss1)\n",
    "            Average_loss2=Average_loss2 + tf.reduce_mean(Loss2)\n",
    "    Average_loss1 = Average_loss1/count\n",
    "    Average_loss2 = Average_loss2/count\n",
    "    History.append([epoch, Average_loss1,Average_loss2])\n",
    "    tf.print(\"=>Epoch%4d  loss1:%4.6f loss2:%4.6f\" %(epoch, Average_loss1,Average_loss2))   \n",
    "\n",
    "    model.save_weights(\"./Fatigue_PINN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "model  = Fatigue_PINN()\n",
    "model.load_weights(\"./Fatigue_PINN\")\n",
    "X_data_test = [] \n",
    "X_data_increase_test = []\n",
    "X_data_factor_test = [] \n",
    "Y_data_test = []\n",
    "Y_data_con_test =[]\n",
    "for i in range (len(degree25_400_5)-15):\n",
    "    X_data_test.append(np.transpose([np.array(degree25_400_5.iloc[i:i+15,1])/step_max,np.array(degree25_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor_test.append([[0,0,1],[0,0,1]])\n",
    "    X_data_increase_test.append(degree25_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data_test.append(degree25_400_5.iloc[i+15,0])\n",
    "    Y_data_con_test.append([1-degree25_para[0]*(degree25_400_5.iloc[i+15,1]**degree25_para[1])-0.1,1-degree25_para[0]*(degree25_400_5.iloc[i+15,1]**degree25_para[1])+0.1])\n",
    "\n",
    "X_data_test = np.array(X_data_test) \n",
    "X_data_increase_test = np.array(X_data_increase_test)\n",
    "X_data_factor_test = np.array(X_data_factor_test) \n",
    "Y_data_test = np.array(Y_data_test)\n",
    "Y_data_con_test = np.array(Y_data_con_test)\n",
    "    \n",
    "predicted_results=[]\n",
    "for i in range(len(X_data_test)):\n",
    "    Output1,Output2 = model([X_data_test[i:i+1],X_data_increase_test[i:i+1],X_data_factor_test[i:i+1]]) \n",
    "    predicted_results.append(Output2[:,0]+(Output2[:,1]-Output2[:,0])*Output1)\n",
    "    \n",
    "plt.scatter(np.squeeze(Y_data_test),np.squeeze(predicted_results))\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ede1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####绘制曲线######\n",
    "model  = Fatigue_PINN()\n",
    "model.load_weights(\"./Fatigue_PINN\")\n",
    "initial_data_x = np.zeros(len(degree25_400_5))\n",
    "initial_data_x[0:20] = degree25_400_5.iloc[0:20,0]\n",
    "initial_data_inc = degree25_400_5.iloc[:,1]/step_max\n",
    "for i in range(20,len(initial_data_inc)):\n",
    "    X_data_test = []\n",
    "    X_data_factor_test =[]\n",
    "    X_data_increase_test=[]\n",
    "    predicted_result = []\n",
    "    for j in range(11):\n",
    "        tem =np.random.choice(i, 15, replace=False)\n",
    "        tem.sort()\n",
    "        X_data_test.append(np.transpose([np.array(degree25_400_5.iloc[tem,1])/step_max,np.array(initial_data_x[tem])]))\n",
    "        X_data_factor_test.append([[0,0,1],[0,0,1]])\n",
    "        X_data_increase_test.append(degree25_400_5.iloc[i,1]/step_max)\n",
    "    X_data_test = np.array(X_data_test)\n",
    "    X_data_factor_test = np.array(X_data_factor_test)\n",
    "    X_data_increase_test= np.array(X_data_increase_test)\n",
    "    Output1,Output2 = model([X_data_test,X_data_increase_test,X_data_factor_test]) \n",
    "    predicted_result.append(Output2[:,0]+(Output2[:,1]-Output2[:,0])*Output1\n",
    "    initial_data_x[i] = np.median(predicted_result)\n",
    "\n",
    "plt.plot(initial_data_inc*step_max,initial_data_x)\n",
    "plt.plot(initial_data_inc*step_max,degree25_400_5.iloc[:,0])\n",
    "plt.show()\n",
    "Ontest_set = np.concatenate((np.array(initial_data_inc*step_max)[...,np.newaxis],\n",
    "                             np.array(initial_data_x)[...,np.newaxis],\n",
    "                             np.array(degree25_400_5.iloc[:,0])[...,np.newaxis]),-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
