{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import time\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity\n",
    "import time\n",
    "from skimage import filters, img_as_ubyte,morphology,measure\n",
    "from scipy.ndimage import label\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13858b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############数据导入###################\n",
    "degree5_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-200-5.csv\").iloc[1:,1:]\n",
    "degree5_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-300-5.csv\").iloc[1:,1:]\n",
    "degree5_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-400-5.csv\").iloc[1:,1:]\n",
    "degree15_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-200-5.csv\").iloc[1:,1:]\n",
    "degree15_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-300-5.csv\").iloc[1:,1:]\n",
    "degree15_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-400-5.csv\").iloc[1:,1:]\n",
    "degree25_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-200-5.csv\").iloc[1:,1:]\n",
    "degree25_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-300-5.csv\").iloc[1:,1:]\n",
    "degree25_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-400-5.csv\").iloc[1:,1:] ###测试\n",
    "\n",
    "step_max = int(np.array([degree5_200_5.max(),degree5_300_5.max(),degree5_400_5.max(),\n",
    "            degree15_200_5.max(),degree15_300_5.max(),degree15_400_5.max(),\n",
    "            degree25_200_5.max(),degree25_300_5.max(),degree25_400_5.max(),]).max())+500\n",
    "####建立训练集\n",
    "###lenght=15, 预测1个,随即时间间隔+物理约束\n",
    "####5,15,25三类，200，300，400三类\n",
    "X_data = [] \n",
    "X_data_increase = []\n",
    "X_data_factor = [] \n",
    "Y_data = []\n",
    "\n",
    "########5度###############\n",
    "for i in range (len(degree5_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_200_5.iloc[i:i+15,1])/step_max,np.array(degree5_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[1,0,0]])\n",
    "    X_data_increase.append(degree5_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_300_5.iloc[i:i+15,1])/step_max,np.array(degree5_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,1,0]])\n",
    "    X_data_increase.append(degree5_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_400_5.iloc[i:i+15,1])/step_max,np.array(degree5_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,0,1]])\n",
    "    X_data_increase.append(degree5_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_400_5.iloc[i+15,0])\n",
    "\n",
    "########15度###############\n",
    "for i in range (len(degree15_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_200_5.iloc[i:i+15,1])/step_max,np.array(degree15_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[1,0,0]])\n",
    "    X_data_increase.append(degree15_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_300_5.iloc[i:i+15,1])/step_max,np.array(degree15_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,1,0]])\n",
    "    X_data_increase.append(degree15_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_400_5.iloc[i:i+15,1])/step_max,np.array(degree15_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,0,1]])\n",
    "    X_data_increase.append(degree15_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_400_5.iloc[i+15,0])\n",
    "\n",
    "########25度###############\n",
    "for i in range (len(degree25_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_200_5.iloc[i:i+15,1])/step_max,np.array(degree25_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[1,0,0]])\n",
    "    X_data_increase.append(degree25_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree25_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_300_5.iloc[i:i+15,1])/step_max,np.array(degree25_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[0,1,0]])\n",
    "    X_data_increase.append(degree25_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_300_5.iloc[i+15,0])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "X_data_increase = np.array(X_data_increase)\n",
    "Y_data = np.array(Y_data)\n",
    "X_data_factor = np.array(X_data_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ANN######\n",
    "def Fatigue_CNN():\n",
    "    History_input = tf.keras.layers.Input(shape=(15,2)) #历史数据，15个点\n",
    "    Predicted_input_x = tf.keras.layers.Input(shape=(1)) #预测点的x坐标\n",
    "    Class_curve = tf.keras.layers.Input(shape=(2,3)) #判定曲线类型\n",
    "    \n",
    "    input1 = tf.keras.layers.Flatten()(History_input)\n",
    "    input2 = tf.keras.layers.Dense(64)(Predicted_input_x)\n",
    "    input3 = tf.keras.layers.Dense(64)(Class_curve)\n",
    "    input3 = tf.keras.layers.Flatten()(input3)\n",
    "    \n",
    "    Sum = tf.keras.layers.Dense((64*64*3))(tf.concat([input1,input2,input3],-1))\n",
    "    Reshape = tf.keras.layers.Reshape((64,64,3))(Sum)\n",
    "   \n",
    "    Conv2D_1 = tf.keras.layers.Conv2D(64,3,1,activation=tf.nn.leaky_relu,padding='same')(Reshape)\n",
    "    Conv2D_2 = tf.keras.layers.Conv2D(64,3,1,activation=tf.nn.leaky_relu,padding='same')(Conv2D_1)\n",
    "    Downsample_1 = tf.keras.layers.Conv2D(64,3,2,activation=tf.nn.leaky_relu,padding='same')(Conv2D_2)\n",
    "    Conv2D_3 = tf.keras.layers.Conv2D(64,3,1,activation=tf.nn.leaky_relu,padding='same')(Downsample_1)\n",
    "    Conv2D_4 = tf.keras.layers.Conv2D(64,3,1,activation=tf.nn.leaky_relu,padding='same')(Conv2D_3)\n",
    "    Downsample_2 = tf.keras.layers.Conv2D(64,3,2,activation=tf.nn.leaky_relu,padding='same')(Conv2D_4 )\n",
    "    Conv2D_5 = tf.keras.layers.Conv2D(64,3,1,activation=tf.nn.leaky_relu,padding='same')(Downsample_2)\n",
    "    Conv2D_6 = tf.keras.layers.Conv2D(64,3,1,activation=tf.nn.leaky_relu,padding='same')(Conv2D_5)\n",
    "    Maxpooling = tf.keras.layers.MaxPool2D((2,2),(1,1))(Conv2D_6)\n",
    "    flatten =tf.keras.layers.Flatten()(Maxpooling)\n",
    "\n",
    "    Dense1 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(flatten)\n",
    "    Dense2 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense1)\n",
    "    Output = tf.keras.layers.Dense(1,activation='relu')(Dense2) ###输出预测\n",
    "    \n",
    "    #固化模型 \n",
    "    model=tf.keras.models.Model(inputs=[History_input,Predicted_input_x,Class_curve],outputs=Output)   \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ee96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=Fatigue_CNN()\n",
    "model.compile( optimizer=tf.keras.optimizers.Adam(1e-4),loss=tf.keras.losses.MeanAbsoluteError())\n",
    "History = model.fit(x=[X_data,X_data_increase,X_data_factor],y=Y_data,batch_size=4,epochs=100)\n",
    "model.save_weights(\"./Fatigue_CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc312426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "model = Fatigue_CNN()\n",
    "model.load_weights(\"./Fatigue_CNN\")\n",
    "X_data_test = [] \n",
    "X_data_increase_test = []\n",
    "X_data_factor_test = [] \n",
    "Y_data_test = []\n",
    "for i in range (len(degree25_400_5)-15):\n",
    "    X_data_test.append(np.transpose([np.array(degree25_400_5.iloc[i:i+15,1])/step_max,np.array(degree25_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor_test.append([[0,0,1],[0,0,1]])\n",
    "    X_data_increase_test.append(degree25_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data_test.append(degree25_400_5.iloc[i+15,0])\n",
    "\n",
    "X_data_test = np.array(X_data_test) \n",
    "X_data_increase_test = np.array(X_data_increase_test)\n",
    "X_data_factor_test = np.array(X_data_factor_test)\n",
    "Y_data_test = np.array(Y_data_test)\n",
    "    \n",
    "predicted_results=[]\n",
    "for i in range(len(X_data_test)):\n",
    "    predicted_results.append(model([X_data_test[i:i+1],X_data_increase_test[i:i+1],X_data_factor_test[i:i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.squeeze(Y_data_test),np.squeeze(predicted_results))\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157681ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####绘制曲线######\n",
    "model = Fatigue_CNN()\n",
    "model.load_weights(\"./Fatigue_CNN\")\n",
    "initial_data_x = np.zeros(len(degree25_400_5))\n",
    "initial_data_x[0:20] = degree25_400_5.iloc[0:20,0]\n",
    "initial_data_inc = degree25_400_5.iloc[:,1]/step_max\n",
    "for i in range(20,len(initial_data_inc)):\n",
    "    X_data_test = np.transpose([np.array(degree25_400_5.iloc[i-15:i,1])/step_max,np.array(degree25_400_5.iloc[i-15:i,0])])[np.newaxis,...]\n",
    "    X_data_increase_test = np.array(degree25_400_5.iloc[i,1]/step_max)[np.newaxis,...]\n",
    "    X_data_factor_test_test = np.array([[0,0,1],[0,0,1]])[np.newaxis,...]\n",
    "    initial_data_x[i]= model([X_data_test,X_data_increase_test,X_data_factor_test_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(initial_data_inc*step_max,initial_data_x)\n",
    "plt.plot(initial_data_inc*step_max,degree25_400_5.iloc[:,0])\n",
    "plt.show()\n",
    "Ontest_set = np.concatenate((np.array(initial_data_inc*step_max)[...,np.newaxis],\n",
    "                             np.array(initial_data_x)[...,np.newaxis],\n",
    "                             np.array(degree25_400_5.iloc[:,0])[...,np.newaxis]),-1)\n",
    "Ontest_set = pd.DataFrame(Ontest_set)\n",
    "Ontest_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/CNN_testset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1190b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "predicted_results=[]\n",
    "for i in range(len(X_data)):\n",
    "    predicted_results.append(model([X_data[i:i+1],X_data_increase[i:i+1],X_data_factor[i:i+1]]))\n",
    "    \n",
    "ON_train_set = np.concatenate((np.squeeze(Y_data)[...,np.newaxis],np.squeeze(predicted_results)[...,np.newaxis]),-1)\n",
    "ON_train_set = pd.DataFrame(ON_train_set)\n",
    "ON_train_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/CNN_trainset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8cba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
