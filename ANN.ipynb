{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fca176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import time\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity\n",
    "import time\n",
    "from skimage import filters, img_as_ubyte,morphology,measure\n",
    "from scipy.ndimage import label\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "################数据导入###################\n",
    "degree5_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-200-5.csv\").iloc[:,1:]\n",
    "degree5_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-300-5.csv\").iloc[:,1:]\n",
    "degree5_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\5-400-5.csv\").iloc[:,1:]\n",
    "degree15_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-200-5.csv\").iloc[:,1:]\n",
    "degree15_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-300-5.csv\").iloc[:,1:]\n",
    "degree15_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\15-400-5.csv\").iloc[:,1:]\n",
    "degree25_200_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-200-5.csv\").iloc[:,1:]\n",
    "degree25_300_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-300-5.csv\").iloc[:,1:]\n",
    "degree25_400_5 = pd.read_csv(r\"D:\\AI in NTU\\PINN-fatigue\\论文数据汇总\\5hzC-S数据\\25-400-5.csv\").iloc[:,1:] ###测试\n",
    "\n",
    "step_max = int(np.array([degree5_200_5.max(),degree5_300_5.max(),degree5_400_5.max(),\n",
    "            degree15_200_5.max(),degree15_300_5.max(),degree15_400_5.max(),\n",
    "            degree25_200_5.max(),degree25_300_5.max(),degree25_400_5.max(),]).max())+500\n",
    "####建立训练集\n",
    "###lenght=15, 预测1个,随即时间间隔+物理约束\n",
    "####5,15,25三类，200，300，400三类\n",
    "X_data = [] \n",
    "X_data_increase = []\n",
    "X_data_factor = [] \n",
    "Y_data = []\n",
    "\n",
    "########5度###############\n",
    "for i in range (len(degree5_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_200_5.iloc[i:i+15,1])/step_max,np.array(degree5_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[1,0,0]])\n",
    "    X_data_increase.append(degree5_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_300_5.iloc[i:i+15,1])/step_max,np.array(degree5_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,1,0]])\n",
    "    X_data_increase.append(degree5_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree5_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree5_400_5.iloc[i:i+15,1])/step_max,np.array(degree5_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[1,0,0],[0,0,1]])\n",
    "    X_data_increase.append(degree5_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree5_400_5.iloc[i+15,0])\n",
    "\n",
    "########15度###############\n",
    "for i in range (len(degree15_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_200_5.iloc[i:i+15,1])/step_max,np.array(degree15_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[1,0,0]])\n",
    "    X_data_increase.append(degree15_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_300_5.iloc[i:i+15,1])/step_max,np.array(degree15_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,1,0]])\n",
    "    X_data_increase.append(degree15_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_300_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree15_400_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree15_400_5.iloc[i:i+15,1])/step_max,np.array(degree15_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,1,0],[0,0,1]])\n",
    "    X_data_increase.append(degree15_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree15_400_5.iloc[i+15,0])\n",
    "\n",
    "########25度###############\n",
    "for i in range (len(degree25_200_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_200_5.iloc[i:i+15,1])/step_max,np.array(degree25_200_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[1,0,0]])\n",
    "    X_data_increase.append(degree25_200_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_200_5.iloc[i+15,0])\n",
    "\n",
    "for i in range (len(degree25_300_5)-15):\n",
    "    X_data.append(np.transpose([np.array(degree25_300_5.iloc[i:i+15,1])/step_max,np.array(degree25_300_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor.append([[0,0,1],[0,1,0]])\n",
    "    X_data_increase.append(degree25_300_5.iloc[i+15,1]/step_max)\n",
    "    Y_data.append(degree25_300_5.iloc[i+15,0])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "X_data_increase = np.array(X_data_increase)\n",
    "Y_data = np.array(Y_data)\n",
    "X_data_factor = np.array(X_data_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fe4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ANN######\n",
    "def Fatigue_ANN():\n",
    "    History_input = tf.keras.layers.Input(shape=(15,2)) #历史数据，15个点\n",
    "    Predicted_input_x = tf.keras.layers.Input(shape=(1)) #预测点的x坐标\n",
    "    Class_curve = tf.keras.layers.Input(shape=(2,3)) #判定曲线类型\n",
    "    \n",
    "    history_input = tf.keras.layers.Flatten()(History_input)\n",
    "    Dense1 = tf.keras.layers.Dense(128,activation=tf.nn.leaky_relu)(history_input)\n",
    "    Dense2 = tf.keras.layers.Dense(128,activation=tf.nn.leaky_relu)(Dense1)\n",
    "    LayerNorm1 = tf.keras.layers.LayerNormalization()(Dense2)\n",
    "    Dense3 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(LayerNorm1)\n",
    "    Dense4 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense3)\n",
    "    \n",
    "    Densex1 = tf.keras.layers.Dense(128)(Predicted_input_x)\n",
    "    Densex2 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Densex1)\n",
    "    \n",
    "    Onehot1 = tf.keras.layers.Dense(64)(Class_curve)\n",
    "    Onehot1 = tf.keras.layers.Flatten()(Onehot1)\n",
    "    Denses1 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Onehot1)\n",
    "    \n",
    "    Dense_s1 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(tf.concat([Dense4,Densex2,Denses1],-1))\n",
    "    Dense_s2 = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense_s1)\n",
    "    Output = tf.keras.layers.Dense(1,activation='relu')(Dense_s2) ###输出预测\n",
    "    \n",
    "    #固化模型 \n",
    "    model=tf.keras.models.Model(inputs=[History_input,Predicted_input_x,Class_curve],outputs=Output)   \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5238b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Fatigue_ANN()\n",
    "model.compile( optimizer='adam',loss=tf.keras.losses.MeanAbsoluteError())\n",
    "History = model.fit(x=[X_data,X_data_increase,X_data_factor],y=Y_data,batch_size=4,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./Fatigue_ANN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16226a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "model = Fatigue_ANN()\n",
    "model.load_weights(\"./Fatigue_ANN\")\n",
    "X_data_test = [] \n",
    "X_data_increase_test = []\n",
    "X_data_factor_test = [] \n",
    "Y_data_test = []\n",
    "for i in range (len(degree25_400_5)-15):\n",
    "    X_data_test.append(np.transpose([np.array(degree25_400_5.iloc[i:i+15,1])/step_max,np.array(degree25_400_5.iloc[i:i+15,0])]))\n",
    "    X_data_factor_test.append([[0,0,1],[0,0,1]])\n",
    "    X_data_increase_test.append(degree25_400_5.iloc[i+15,1]/step_max)\n",
    "    Y_data_test.append(degree25_400_5.iloc[i+15,0])\n",
    "\n",
    "X_data_test = np.array(X_data_test) \n",
    "X_data_increase_test = np.array(X_data_increase_test)\n",
    "X_data_factor_test = np.array(X_data_factor_test)\n",
    "Y_data_test = np.array(Y_data_test)\n",
    "    \n",
    "predicted_results=[]\n",
    "for i in range(len(X_data_test)):\n",
    "    predicted_results.append(model([X_data_test[i:i+1],X_data_increase_test[i:i+1],X_data_factor_test[i:i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.squeeze(Y_data_test),np.squeeze(predicted_results))\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####绘制曲线######\n",
    "model = Fatigue_ANN()\n",
    "model.load_weights(\"./Fatigue_ANN\")\n",
    "initial_data_x = np.zeros(len(degree25_400_5))\n",
    "initial_data_x[0:20] = degree25_400_5.iloc[0:20,0]\n",
    "initial_data_inc = degree25_400_5.iloc[:,1]/step_max\n",
    "for i in range(20,len(initial_data_inc)):\n",
    "    X_data_test = np.transpose([np.array(degree25_400_5.iloc[i-15:i,1])/step_max,np.array(degree25_400_5.iloc[i-15:i,0])])[np.newaxis,...]\n",
    "    X_data_increase_test = np.array(degree25_400_5.iloc[i,1]/step_max)[np.newaxis,...]\n",
    "    X_data_factor_test_test = np.array([[0,0,1],[0,0,1]])[np.newaxis,...]\n",
    "    initial_data_x[i]= model([X_data_test,X_data_increase_test,X_data_factor_test_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(initial_data_inc*step_max,initial_data_x)\n",
    "plt.plot(initial_data_inc*step_max,degree25_400_5.iloc[:,0])\n",
    "plt.show()\n",
    "Ontest_set = np.concatenate((np.array(initial_data_inc*step_max)[...,np.newaxis],\n",
    "                             np.array(initial_data_x)[...,np.newaxis],\n",
    "                             np.array(degree25_400_5.iloc[:,0])[...,np.newaxis]),-1)\n",
    "Ontest_set = pd.DataFrame(Ontest_set)\n",
    "Ontest_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/ANN_testset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############for test#################\n",
    "predicted_results=[]\n",
    "for i in range(len(X_data)):\n",
    "    predicted_results.append(model([X_data[i:i+1],X_data_increase[i:i+1],X_data_factor[i:i+1]]))\n",
    "    \n",
    "ON_train_set = np.concatenate((np.squeeze(Y_data)[...,np.newaxis],np.squeeze(predicted_results)[...,np.newaxis]),-1)\n",
    "ON_train_set = pd.DataFrame(ON_train_set)\n",
    "ON_train_set.to_excel(\"D:/AI in NTU/PINN-fatigue/实验结果汇总/ANN_trainset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cef3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
